{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Represent a word as a vector\n",
    "  - Two related words should have two vectors close by.\n",
    "* Data. \n",
    "  - 全唐诗 + 全宋词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Embedding of words (characters)\n",
    "  - E.g., words related semanticly should have close-by embedding vector representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='data/linear-relationships.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $p(w, c; \\theta)$ is modeled as\n",
    "$$\n",
    "p(w, c; \\theta) = \\sigma(v_w^T v_c)\n",
    "$$\n",
    "\n",
    "* $\\sigma(x)$ is the sigmoid function: $\\frac{1}{1+e^{-x}}$\n",
    "* $v_w^T v_c$ is the dot product of embedding vectors of $w$ and $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $p(w, c; \\theta)$ denotes the probability a word ($w$) and a context ($c$) appears in the training data.\n",
    "* Objective:\n",
    "$$ \n",
    "\\operatorname*{arg\\,max}_\\theta \\prod_{(w,c) \\in D} p(w, c; \\theta)\\prod_{(w,c) \\notin D} (1-p(w, c; \\theta))\n",
    "$$\n",
    "$$ \n",
    "= \\operatorname*{arg\\,min}_\\theta [-\\sum_{(w,c) \\in D} log(p(w, c; \\theta)) - \\sum_{(w,c) \\notin D} log((1-p(w, c; \\theta)))]\n",
    "$$\n",
    "* $(w,c) \\notin D$ is called negative examples. \n",
    "  - Generated randomly. Left as an exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class TrainData(object):\n",
    "\n",
    "    def __init__(self, corpus, batch, windows):\n",
    "        self.batch = batch\n",
    "        self.windows = windows\n",
    "        words = open(corpus, mode='r').read()\n",
    "        words = words.replace('。', '').replace('，', '').replace('\\n', '')\n",
    "        words_as_set = set(words)\n",
    "        self.id_to_word = sorted(set(words))\n",
    "        self.word_to_id = {w: i for i, w in enumerate(self.id_to_word)}\n",
    "        self.data = [self.word_to_id[w] for w in words]\n",
    "        print('Number of unique chars: ', len(self.id_to_word))\n",
    "        print('Number of training chars: ', len(self.data))\n",
    "        self.seqgen = self.skipgram_generator()\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return len(self.id_to_word)\n",
    "    \n",
    "    def skipgram_generator(self):\n",
    "        curr = 0\n",
    "        while True:\n",
    "            curr %= len(self.data)\n",
    "            x = self.data[curr]\n",
    "            left = self.data[max(0, curr - self.windows):curr]\n",
    "            right = self.data[curr + 1:curr + 1 + self.windows]\n",
    "            for y in left + right:\n",
    "                yield (x, y)\n",
    "            curr += 1\n",
    "            \n",
    "    def get_batch(self):\n",
    "        input, target = [], []\n",
    "        for _ in range(self.batch):\n",
    "            x, y = next(self.seqgen)\n",
    "            input.append(x)\n",
    "            target.append(y)\n",
    "        return np.array(input), np.array(target)\n",
    "    \n",
    "    def to_ids(self, words):\n",
    "        ids = []\n",
    "        for w in words:\n",
    "            if w in self.word_to_id:\n",
    "                ids.append(self.word_to_id[w])\n",
    "        return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class TrainData(object):\n",
    "\n",
    "    def __init__(self, corpus, batch, windows):\n",
    "        self.batch = batch\n",
    "        self.windows = windows\n",
    "        words = open(corpus, mode='r').read()\n",
    "        words = words.replace('。', '').replace('，', '').replace('\\n', '')\n",
    "        words_as_set = set(words)\n",
    "        self.id_to_word = sorted(set(words))\n",
    "        self.word_to_id = {w: i for i, w in enumerate(self.id_to_word)}\n",
    "        self.data = [self.word_to_id[w] for w in words]\n",
    "        print('Number of unique chars: ', len(self.id_to_word))\n",
    "        print('Number of training chars: ', len(self.data))\n",
    "        self.seqgen = self.skipgram_generator()\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return len(self.id_to_word)\n",
    "    \n",
    "    def to_ids(self, words):\n",
    "        ids = []\n",
    "        for w in words:\n",
    "            if w in self.word_to_id:\n",
    "                ids.append(self.word_to_id[w])\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "    def skipgram_generator(self):\n",
    "        curr = 0\n",
    "        while True:\n",
    "            curr %= len(self.data)\n",
    "            x = self.data[curr]\n",
    "            left = self.data[max(0, curr - self.windows):curr]\n",
    "            right = self.data[curr + 1:curr + 1 + self.windows]\n",
    "            for y in left + right:\n",
    "                yield (x, y)\n",
    "            curr += 1\n",
    "            \n",
    "    def get_batch(self):\n",
    "        input, target = [], []\n",
    "        for _ in range(self.batch):\n",
    "            x, y = next(self.seqgen)\n",
    "            input.append(x)\n",
    "            target.append(y)\n",
    "        return np.array(input), np.array(target)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique chars:  7955\n",
      "Number of training chars:  3987351\n"
     ]
    }
   ],
   "source": [
    "data = TrainData('./data/poem.txt', 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4562, 4562]), array([1707, 7209]))\n",
      "(array([1707, 1707]), array([4562, 7209]))\n",
      "(array([1707, 7209]), array([1746, 4562]))\n",
      "(array([7209, 7209]), array([1707, 1746]))\n",
      "(array([7209, 1746]), array([1472, 1707]))\n",
      "(array([1746, 1746]), array([7209, 1472]))\n",
      "(array([1746, 1472]), array([ 552, 7209]))\n",
      "(array([1472, 1472]), array([1746,  552]))\n",
      "(array([1472,  552]), array([6301, 1746]))\n",
      "(array([552, 552]), array([1472, 6301]))\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(data.get_batch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implement the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, dims, vocab, lr):\n",
    "        # Configs.\n",
    "        self.dims = dims\n",
    "        self.vocab = vocab\n",
    "        self.lr = lr\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Var\n",
    "            self.embedding = tf.Variable(\n",
    "                tf.random_uniform([vocab, dims], -0.02, 0.02))\n",
    "\n",
    "            # Feeds.\n",
    "            self.inputs = tf.placeholder(tf.int64)\n",
    "            self.targets = tf.placeholder(tf.int64)\n",
    "\n",
    "            # Define forward.\n",
    "            x_emb = tf.nn.embedding_lookup(self.embedding, self.inputs)\n",
    "            y_emb = tf.nn.embedding_lookup(self.embedding, self.targets)\n",
    "\n",
    "            # Compute the loss.\n",
    "            scores = tf.reduce_sum(x_emb * y_emb, [1])\n",
    "            probs = tf.sigmoid(scores)\n",
    "            logp = tf.log(probs)\n",
    "            self.loss = - tf.reduce_mean(logp)\n",
    "\n",
    "            # Define training.\n",
    "            self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            vars = tf.trainable_variables()\n",
    "            grads = tf.gradients(self.loss, vars)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            self.train_op = optimizer.apply_gradients(\n",
    "                zip(grads, vars), global_step=self.global_step)\n",
    "\n",
    "            # Summary\n",
    "            tf.scalar_summary('loss', self.loss)\n",
    "            self.summary = tf.merge_summary(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "            # Inference\n",
    "\n",
    "            # Nearest neighbors\n",
    "            norm_embs = tf.nn.l2_normalize(self.embedding, 1)\n",
    "            word_embs = tf.nn.embedding_lookup(norm_embs, self.inputs)\n",
    "            distance = tf.matmul(word_embs, norm_embs, transpose_b=True)\n",
    "            self.neighbors_topk = tf.nn.top_k(distance, k=10)\n",
    "\n",
    "            # Analogy\n",
    "            a, b, c = word_embs[1, :], word_embs[0, :], word_embs[2, :]\n",
    "            d = b - a + c\n",
    "            target = tf.reshape(d, [1, -1])\n",
    "            dist = tf.matmul(target, norm_embs, transpose_b=True)\n",
    "            self.analogy_topk = tf.nn.top_k(dist, k=10)\n",
    "\n",
    "            # Init\n",
    "            self.init = tf.initialize_all_variables()\n",
    "\n",
    "            # Saver\n",
    "            self.saver = tf.train.Saver(tf.all_variables())\n",
    "            \n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "    def train(self, data, logdir, total_steps):\n",
    "        swriter = tf.train.SummaryWriter(logdir)\n",
    "        \n",
    "        # Recover.\n",
    "        self.load(tf.train.latest_checkpoint(logdir))\n",
    "        \n",
    "        steps = self.sess.run(self.global_step)\n",
    "        while steps < total_steps:\n",
    "            if steps % 1000 == 0:\n",
    "                self.saver.save(\n",
    "                    self.sess, logdir + '/wv_params', global_step=steps)\n",
    "            x, y = data.get_batch()\n",
    "            if steps % 100 == 0:\n",
    "                loss, summary = self.sess.run(\n",
    "                    [self.loss, self.summary],\n",
    "                    feed_dict={self.inputs: x, self.targets: y})\n",
    "                swriter.add_summary(summary, steps)\n",
    "                swriter.flush()\n",
    "                print('step %d: %.4f' % (steps, loss))\n",
    "            else:\n",
    "                self.sess.run(\n",
    "                    self.train_op,\n",
    "                    feed_dict={self.inputs: x, self.targets: y})\n",
    "            steps += 1\n",
    "\n",
    "    def load(self, checkpoint):\n",
    "        if checkpoint is not None:\n",
    "            print('restore %s', checkpoint)\n",
    "            self.saver.restore(self.sess, checkpoint)\n",
    "            \n",
    "    def nearby(self, data, words):\n",
    "        ids = data.to_ids(words)\n",
    "        print('ids = %s' % ids)\n",
    "        _, neighbors = self.sess.run(\n",
    "            self.neighbors_topk, feed_dict={self.inputs : ids})\n",
    "        for (w, n) in zip(words, neighbors):\n",
    "            print('nearby  %s --> %s' % (w, ''.join(\n",
    "                        [data.id_to_word[x] for x in n])))\n",
    "            \n",
    "    def analogy(self, data, words):\n",
    "        _, neighbors = self.sess.run(\n",
    "            self.analogy_topk,\n",
    "            feed_dict={self.inputs : data.to_ids(words)})\n",
    "        neighbors = [data.id_to_word[x] for x in neighbors[0, :]]\n",
    "        print('analogy %s %s' % (words, ''.join(neighbors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Parameters of the model\n",
    "  * Embedding dimensions\n",
    "    - dims = 256\n",
    "  * Vocab size\n",
    "    - data.vocab = 7955\n",
    "  * Learning rate for SGD\n",
    "    - lr = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, dims, vocab, lr):\n",
    "        # Configs.\n",
    "        self.dims = dims\n",
    "        self.vocab = vocab\n",
    "        self.lr = lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Describe the model as a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Declare embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Var\n",
    "            self.embedding = tf.Variable(\n",
    "                tf.random_uniform([vocab, dims], -0.02, 0.02))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feeds: inputs to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Feeds.\n",
    "            self.inputs = tf.placeholder(tf.int64)\n",
    "            self.targets = tf.placeholder(tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward function\n",
    "  - Really simple for this model. Just converts word ids into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Define forward.\n",
    "            x_emb = tf.nn.embedding_lookup(self.embedding, self.inputs)\n",
    "            y_emb = tf.nn.embedding_lookup(self.embedding, self.targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss: the optimization goal\n",
    "$$ -\\sum_{(w,c) \\in D} log(p(w, c; \\theta)) $$\n",
    "\n",
    "$$\n",
    "p(w, c; \\theta) = \\sigma(v_w^T v_c)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Compute the loss.\n",
    "            scores = tf.reduce_sum(x_emb * y_emb, [1])\n",
    "            probs = tf.sigmoid(scores) \n",
    "            logp = tf.log(probs)\n",
    "            self.loss = - tf.reduce_mean(logp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizer: how to minimize the loss\n",
    "  * Stochastic gradient descent.\n",
    "    - Only hyper-parameter: learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Define training.\n",
    "            self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            vars = tf.trainable_variables()\n",
    "            grads = tf.gradients(self.loss, vars)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            self.train_op = optimizer.apply_gradients(\n",
    "                zip(grads, vars), global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training support\n",
    "  - Summary: pretty plots over time\n",
    "  - Saver: checkpoint trainig state\n",
    "  - Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Summary\n",
    "            tf.scalar_summary('loss', self.loss)\n",
    "            self.summary = tf.merge_summary(\n",
    "                tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "            # Init\n",
    "            self.init = tf.initialize_all_variables()\n",
    "\n",
    "            # Saver\n",
    "            self.saver = tf.train.Saver(tf.all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference\n",
    "  * Similiar words for a given word\n",
    "  * Analogy: A vs. B as C vs. ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Inference\n",
    "\n",
    "            # Nearest neighbors\n",
    "            norm_embs = tf.nn.l2_normalize(self.embedding, 1)\n",
    "            word_embs = tf.nn.embedding_lookup(norm_embs, self.inputs)\n",
    "            distance = tf.matmul(word_embs, norm_embs, transpose_b=True)\n",
    "            self.neighbors_topk = tf.nn.top_k(distance, k=10)\n",
    "\n",
    "            # Analogy\n",
    "            a, b, c = word_embs[1, :], word_embs[0, :], word_embs[2, :]\n",
    "            d = b - a + c\n",
    "            target = tf.reshape(d, [1, -1])\n",
    "            dist = tf.matmul(target, norm_embs, transpose_b=True)\n",
    "            self.analogy_topk = tf.nn.top_k(dist, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Session\n",
    "  - Connect to TensorFlow runtime\n",
    "  - Initialize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "       self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training\n",
    "  - Iterative algorithm\n",
    "  - Periodic checkpoint and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    def train(self, data, logdir, total_steps):\n",
    "        swriter = tf.train.SummaryWriter(logdir)\n",
    "        \n",
    "        # Recover.\n",
    "        self.load(tf.train.latest_checkpoint(logdir))\n",
    "        \n",
    "        steps = self.sess.run(self.global_step)\n",
    "        while steps < total_steps:\n",
    "            if steps % 1000 == 0:\n",
    "                self.saver.save(\n",
    "                    self.sess, logdir + '/wv_params', global_step=steps)\n",
    "            x, y = data.get_batch()\n",
    "            if steps % 100 == 0:\n",
    "                loss, summary = self.sess.run(\n",
    "                    [self.loss, self.summary],\n",
    "                    feed_dict={self.inputs: x, self.targets: y})\n",
    "                swriter.add_summary(summary, steps)\n",
    "                swriter.flush()\n",
    "                print('step %d: %.4f' % (steps, loss))\n",
    "            else:\n",
    "                self.sess.run(\n",
    "                    self.train_op,\n",
    "                    feed_dict={self.inputs: x, self.targets: y})\n",
    "            steps += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "    def load(self, checkpoint):\n",
    "        if checkpoint is not None:\n",
    "            print('restore ', checkpoint)\n",
    "            self.saver.restore(self.sess, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Instantiate the data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique chars:  7955\n",
      "Number of training chars:  3987351\n"
     ]
    }
   ],
   "source": [
    "windows = 8\n",
    "batches = 32\n",
    "data = TrainData('./data/poem.txt', batches, windows)\n",
    "\n",
    "dims = 256\n",
    "vocab = data.vocab\n",
    "lr = 0.5\n",
    "model = Model(dims, vocab, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 0.6931\n",
      "step 100: 0.6933\n"
     ]
    }
   ],
   "source": [
    "model.train(data, './data', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore %s ./data/wv_params-0\n",
      "ids = [2623, 2723, 540, 2611, 2724]\n",
      "nearby  明 --> 明蚴檠咸眄垒尧眴牣锺\n",
      "nearby  月 --> 月唏齰麦疗妠嚬泾耎鎗\n",
      "nearby  几 --> 几咮柑臆祲允募咛涤憪\n",
      "nearby  时 --> 时须遣搵汐遗荣磬鞨翛\n",
      "nearby  有 --> 有贡鹦襋骠躔姣ㄒ缙疃\n"
     ]
    }
   ],
   "source": [
    "model.load('./data/wv_params-0')\n",
    "model.nearby(data, '明月几时有')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore %s ./data/wv_params-807000\n",
      "ids = [2623, 2723, 540, 2611, 2724]\n",
      "nearby  明 --> 明中道下上不水日月心\n",
      "nearby  月 --> 月风下明夜落上归日起\n",
      "nearby  几 --> 几不一上汉下西何来知\n",
      "nearby  时 --> 时不相灵日东君如在当\n",
      "nearby  有 --> 有无不何中山人青心门\n"
     ]
    }
   ],
   "source": [
    "model.load('./data/wv_params-807000')\n",
    "model.nearby(data, '明月几时有')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analogy\n",
    "  - 清:风 vs. 明:?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore %s ./data/wv_params-0\n",
      "analogy 清风明 清明撚尧隔檠玺纥鳢猰\n"
     ]
    }
   ],
   "source": [
    "model.load('./data/wv_params-0')\n",
    "model.analogy(data, '清风明')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore %s ./data/wv_params-807000\n",
      "analogy 清风明 明清中德天归得心如思\n"
     ]
    }
   ],
   "source": [
    "model.load('./data/wv_params-807000')\n",
    "model.analogy(data, '清风明')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "  - Train a bigger model.\n",
    "  - Different data set. \n",
    "    - http://mattmahoney.net/dc/text8.zip\n",
    "  - Adds negative samples\n",
    "    - "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
